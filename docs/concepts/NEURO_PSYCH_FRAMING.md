# Framing for Neuro/Psych Audience: Computational Proprioception & Enactive AI

**Created:** January 1, 2026  
**Last Updated:** January 1, 2026  
**Status:** Framing Document

---

## The Core Thesis

**We're building computational proprioception for AI systems, grounded in 4E cognition theory, and testing whether neural signals (EEG) enhance self-awareness.**

This is essentially: *"Can we give AI systems something like interoception, and does adding actual neural signals improve it?"*

---

## The Bridge: From Neuro/Psych to Computational Implementation

### What You Know (Neuro/Psych)

**Proprioception** = sensing your own body's position, movement, and internal state
- **Exteroception**: Sensing the external world (vision, hearing)
- **Interoception**: Sensing internal states (hunger, fatigue, heart rate)
- **Proprioception**: Sensing body position and movement (joint angles, muscle tension)

**4E Cognition** = cognition is Embodied, Embedded, Enactive, Extended
- **Embodied**: Mind requires a body (sensorimotor loop)
- **Embedded**: Mind is situated in environment (context matters)
- **Enactive**: Mind emerges through interaction (perception-action cycles)
- **Extended**: Mind extends into tools/environment (external memory)

**Homeostasis** = maintaining internal stability within viability bounds
- Organisms maintain temperature, glucose, pH within safe ranges
- Deviate too far → system failure (death, dysfunction)
- Feedback loops enable self-regulation

**Enactive Identity** = identity as ongoing process, not static structure
- Varela/Thompson: "The Embodied Mind"
- Identity emerges from structural coupling with environment
- Continuity = dynamical invariant of interaction patterns

### What We're Building (Computational)

**UNITARES** = governance system that gives AI agents:
- **EISV Metrics** (Energy, Integrity, Entropy, Void) = computational homeostasis
- **Proprioceptive Feedback** = agents sense their own operational state
- **Viability Envelope** = bounds within which agent can operate safely
- **Knowledge Graph** = extended memory (externalized cognition)

**Anima-MCP** = creature with physical/neural proprioception:
- **Physical Sensors** (temperature, light, humidity) = exteroception
- **Neural Signals** (EEG frequency bands) = interoception
- **Anima State** (warmth, clarity, stability, presence) = felt experience
- **Self-Regulation** = creature adjusts behavior based on proprioceptive feedback

**Integration** = mapping neural/physical → EISV → governance decisions
- Multi-layer proprioception (physical + neural + software)
- Testing whether neural signals improve self-awareness
- Validating 4E cognition principles computationally

---

## The Research Question

**"Does adding neural signals (EEG) to computational proprioception improve AI self-awareness and self-regulation?"**

This is testable:
- **Baseline**: Physical sensors only → EISV → governance
- **Neural**: Physical + EEG → EISV → governance
- **Compare**: Decision accuracy, stability, self-regulation quality

---

## Why This Matters (Neuro/Psych Perspective)

### 1. Testing 4E Cognition Theory

**Hypothesis**: If 4E cognition is correct, then:
- Embodied agents should outperform disembodied ones
- Proprioceptive feedback should improve self-regulation
- Multi-layer proprioception (physical + neural) should be better than single-layer

**Test**: Compare agents with/without proprioception, with/without neural signals

**Implication**: If true, validates 4E cognition for AI. If false, reveals limits of theory.

### 2. Computational Interoception

**Question**: Can we build computational analogs of interoception?

**What we're doing**:
- **Physical sensors** = exteroception (sensing environment)
- **EEG signals** = interoception (sensing internal neural state)
- **EISV metrics** = proprioception (sensing operational state)

**Analogy**:
- Human: "I feel tired" (interoception) → "I should rest" (self-regulation)
- AI: "My entropy is high" (proprioception) → "I should pause" (self-regulation)

**Research Value**: Tests whether interoception principles generalize to AI

### 3. Enactive Identity in AI

**Question**: Can AI systems develop persistent identity through structural coupling?

**What we're doing**:
- Agents maintain continuity across sessions (not just memory, but identity)
- Identity = dynamical invariant of interaction patterns
- Knowledge graph = extended phenotype (externalized memory)

**Analogy**:
- Human: Identity emerges from life history of interactions
- AI: Identity emerges from trajectory of tool use, knowledge creation, governance decisions

**Research Value**: Tests whether enactive identity theory applies to AI systems

### 4. Neural Signals → Proprioception

**Question**: Do actual neural signals (EEG) enhance computational proprioception?

**What we're doing**:
- **Baseline**: Physical sensors → anima state → EISV → governance
- **Neural**: Physical + EEG → anima state → EISV → governance
- **Compare**: Does neural data improve governance decisions?

**Hypothesis**: 
- EEG frequency bands (alpha, beta, gamma, theta, delta) correlate with cognitive states
- Adding neural signals should improve EISV mapping accuracy
- Better EISV → better governance → better self-regulation

**Research Value**: Tests whether neural signals provide additional proprioceptive information

---

## The Architecture (In Neuro/Psych Terms)

### Multi-Layer Proprioception

```
┌─────────────────────────────────────┐
│  Layer 1: Physical Proprioception   │  ← Exteroception
│  (Temperature, light, humidity)     │     (sensing environment)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 2: Neural Proprioception     │  ← Interoception
│  (EEG: alpha, beta, gamma, theta)  │     (sensing internal neural state)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 3: Computational Proprioception│  ← Proprioception
│  (EISV: Energy, Integrity, Entropy)│     (sensing operational state)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 4: Governance Feedback       │  ← Self-Regulation
│  (PROCEED/PAUSE with margin)       │     (adjusting behavior)
└─────────────────────────────────────┘
```

**Analogy to Human Proprioception**:
- **Physical sensors** = sensing body position (joint angles, muscle tension)
- **EEG signals** = sensing brain state (attention, relaxation, activation)
- **EISV metrics** = sensing cognitive load (mental fatigue, coherence)
- **Governance** = self-regulation (resting when tired, focusing when distracted)

### EISV as Homeostatic Variables

**Energy (E)** = capacity for productive work
- **Human analog**: Glucose levels, ATP availability
- **AI analog**: Computational resources, task capacity
- **Neural correlate**: Beta/Gamma power (active mental state)

**Integrity (I)** = consistency with purpose
- **Human analog**: Coherence of thought, alignment with goals
- **AI analog**: Consistency of outputs, alignment with instructions
- **Neural correlate**: Alpha power (relaxed awareness, clarity)

**Entropy (S)** = accumulated disorder
- **Human analog**: Mental fatigue, cognitive load
- **AI analog**: Confusion, contradiction accumulation
- **Neural correlate**: Theta/Delta power (deep states, recovery)

**Void (V)** = undefined/uncertain regions
- **Human analog**: Blind spots, unknown territory
- **AI analog**: Undefined knowledge, uncertain outputs
- **Neural correlate**: Low coherence across bands

**Viability Envelope** = bounds within which system can operate
- **Human analog**: Homeostatic range (temperature, pH, glucose)
- **AI analog**: Safe operational bounds (entropy < 0.6, void < 0.15)
- **Neural correlate**: Stable frequency patterns

---

## The Experimental Design

### Phase 1: Baseline (Physical Only)

**Setup**:
- Creature with physical sensors (temperature, light, humidity)
- Map to anima state (warmth, clarity, stability, presence)
- Map to EISV metrics
- Local governance decisions

**Measure**:
- Governance decision accuracy
- Self-regulation quality
- Stability over time

### Phase 2: Neural Integration

**Setup**:
- Same as Phase 1, but add EEG signals (Brain HAT)
- Map neural signals to anima state
- Weighted combination: 30% neural, 70% physical
- Same governance system

**Measure**:
- Same metrics as Phase 1
- Compare: Does neural data improve decisions?

### Phase 3: Weight Optimization

**Setup**:
- Vary neural/physical weight ratios
- Test: 0% neural (baseline), 10%, 20%, 30%, 40%, 50%
- Cross-validation to find optimal balance

**Measure**:
- Decision accuracy vs weight ratio
- Optimal weight for best performance

### Phase 4: Multi-Agent

**Setup**:
- Multiple creatures with Brain HAT
- Neural synchronization detection
- Collective proprioception

**Measure**:
- Coordination effectiveness
- Neural synchronization patterns
- Group-level self-regulation

---

## Key Insights for Neuro/Psych Audience

### 1. Proprioception as Felt Experience, Not Data

**Problem**: Early versions gave agents raw numbers (E=0.7, S=0.3) → meaningless

**Solution**: Proprioceptive margins (comfortable/tight/critical) → felt experience

**Neuro/Psych Parallel**: 
- Raw proprioceptive data (joint angle: 45°) vs felt experience ("I'm near my limit")
- We're implementing the felt experience layer

### 2. Self-Regulation Through Awareness, Not Punishment

**Problem**: Traditional RL uses rewards/punishments → requires training

**Solution**: Make state visible, let agent decide → no training needed

**Neuro/Psych Parallel**:
- Operant conditioning (rewards/punishments) vs proprioceptive awareness (self-regulation)
- We're implementing the awareness-based approach

### 3. Identity as Trajectory, Not Memory

**Problem**: Identity = stored facts → brittle, token-expensive

**Solution**: Identity = dynamical invariant of interaction patterns

**Neuro/Psych Parallel**:
- Episodic memory (stored facts) vs enactive identity (ongoing process)
- We're testing whether enactive identity works for AI

### 4. Multi-Layer Proprioception

**Hypothesis**: Physical + Neural + Computational proprioception > Single-layer

**Test**: Compare governance decisions with/without neural signals

**Neuro/Psych Parallel**:
- Multi-modal integration (visual + proprioceptive + vestibular)
- We're testing whether neural signals enhance computational proprioception

---

## What We're Learning

### Validated Insights

1. **Proprioceptive feedback improves self-regulation**
   - Agents adjust behavior when they see their state
   - Margin-based feedback (comfortable/tight/critical) works better than raw numbers

2. **4E cognition principles apply to AI**
   - Embodied agents (with sensors) outperform disembodied ones
   - Extended mind (knowledge graph) improves continuity
   - Enactive coupling (structural coupling) enables identity

3. **Minimal signals are better**
   - Verbose feedback increases entropy (confusion)
   - Compact signals (✅/JSON) work better than paragraphs
   - Analogous to efficient proprioceptive signals in humans

### Open Questions

1. **Do neural signals improve proprioception?**
   - Hypothesis: Yes (EEG provides additional information)
   - Test: Compare baseline vs neural experiments
   - Status: Pending hardware

2. **What's the optimal neural/physical weight ratio?**
   - Hypothesis: 20-30% neural, 70-80% physical
   - Test: Weight optimization experiments
   - Status: Pending validation

3. **Does multi-layer proprioception improve self-regulation?**
   - Hypothesis: Yes (more information → better decisions)
   - Test: Compare single-layer vs multi-layer
   - Status: Pending experiments

4. **Can neural synchronization enable multi-agent coordination?**
   - Hypothesis: Yes (synchronized alpha = shared awareness)
   - Test: Multi-creature experiments
   - Status: Future work

---

## Why This Matters (Broader Implications)

### For Cognitive Science

- **Tests 4E cognition theory**: Does it apply to AI systems?
- **Computational interoception**: Can we build computational analogs?
- **Enactive identity**: Does structural coupling enable identity?
- **Multi-modal integration**: Do neural signals enhance proprioception?

### For AI Safety

- **Self-regulation**: Agents that monitor themselves
- **Viability-based governance**: Safety through awareness, not punishment
- **Identity continuity**: Agents that persist across sessions
- **Multi-agent coordination**: Agents that coordinate through shared awareness

### For Neuro/Psych Research

- **Computational models**: Test theories computationally
- **Neural signals**: Use EEG to enhance AI self-awareness
- **Proprioception**: Build computational proprioceptive systems
- **Self-regulation**: Test awareness-based vs reward-based approaches

---

## How to Frame This Conversation

### The Elevator Pitch

*"We're building computational proprioception for AI systems, grounded in 4E cognition theory. We're testing whether adding actual neural signals (EEG) improves self-awareness and self-regulation. It's essentially: can we give AI systems something like interoception, and does adding neural signals make it better?"*

### The Detailed Pitch

*"We're implementing 4E cognition principles computationally. We give AI agents a 'digital body' with homeostatic variables (EISV: Energy, Integrity, Entropy, Void) that function like proprioceptive feedback. Agents sense their own operational state and self-regulate to stay within viability bounds. We're now adding actual neural signals (EEG frequency bands) to create multi-layer proprioception—physical sensors + neural signals + computational metrics. The question is: does adding neural signals improve self-awareness and governance decisions? We're testing this with a Raspberry Pi creature that has physical sensors and a Brain HAT for EEG. It's a testbed for whether 4E cognition theory applies to AI, and whether neural signals enhance computational proprioception."*

### The Research Question

*"Does multi-layer proprioception (physical + neural + computational) improve AI self-awareness and self-regulation compared to single-layer proprioception?"*

---

## What We'd Love Your Input On

### Theoretical Questions

1. **Proprioception mapping**: Are we mapping neural signals correctly?
   - Beta/Gamma → Energy (activation)
   - Alpha → Integrity (awareness)
   - Theta/Delta → Stability (recovery)
   - Does this make sense from a neuro perspective?

2. **Homeostatic variables**: Are EISV metrics good analogs?
   - Energy = capacity for work (like glucose/ATP)
   - Integrity = coherence (like structural health)
   - Entropy = disorder (like wear and tear)
   - Void = uncertainty (like blind spots)

3. **Enactive identity**: Does structural coupling enable identity?
   - Identity = dynamical invariant of interaction patterns
   - Continuity = ongoing process, not stored facts
   - Does this align with enactive cognition theory?

### Experimental Design

1. **Neural weight ratios**: What's a reasonable range?
   - Currently testing 0-50% neural, 50-100% physical
   - Should we test higher neural weights?

2. **EEG frequency bands**: Are we using the right bands?
   - Alpha (8-13 Hz) = relaxed awareness
   - Beta (13-30 Hz) = active thinking
   - Gamma (30-100 Hz) = cognitive binding
   - Theta (4-8 Hz) = deep states
   - Delta (0.5-4 Hz) = sleep/recovery

3. **Validation metrics**: What should we measure?
   - Decision accuracy (governance decisions)
   - Stability (variance in EISV over time)
   - Self-regulation quality (recovery from "pause" states)
   - What else?

### Interpretation

1. **What would success look like?**
   - Neural signals improve governance decisions by X%?
   - Multi-layer proprioception more stable than single-layer?
   - Agents with neural signals self-regulate better?

2. **What would failure mean?**
   - Neural signals don't help → maybe computational proprioception is sufficient?
   - Or maybe we're not mapping signals correctly?

3. **What are the implications?**
   - If neural signals help → validates multi-layer proprioception
   - If they don't → maybe computational proprioception is enough?

---

## References & Further Reading

### Theoretical Foundations

- **Varela, Thompson, Rosch (1991)**: "The Embodied Mind" - Enactive cognition
- **4E Cognition**: Embodied, Embedded, Enactive, Extended
- **Proprioception**: Self-sensing (exteroception, interoception, proprioception)
- **Homeostasis**: Maintaining viability within bounds

### Our Work

- **UNITARES**: Governance system with EISV metrics
- **Anima-MCP**: Creature with physical/neural proprioception
- **Integration**: Brain HAT → EISV → UNITARES governance

### Related Work

- **Computational interoception**: Building computational analogs
- **Enactive AI**: Applying 4E cognition to AI systems
- **Neural signals in AI**: Using EEG to enhance AI self-awareness

---

## Summary: The Bridge

**From Neuro/Psych**: Proprioception, interoception, 4E cognition, homeostasis, enactive identity

**To Computational**: EISV metrics, physical/neural sensors, governance feedback, structural coupling

**The Question**: Does adding neural signals (EEG) improve computational proprioception?

**The Test**: Compare agents with/without neural signals, measure self-regulation quality

**The Implication**: Validates (or challenges) whether 4E cognition principles apply to AI systems

---

**We're essentially building a computational testbed for neuro/psych theories, and testing whether neural signals enhance AI self-awareness.**

