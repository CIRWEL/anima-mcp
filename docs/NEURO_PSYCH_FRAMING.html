<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>NEURO_PSYCH_FRAMING</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@5/github-markdown.min.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">NEURO_PSYCH_FRAMING</h1>
</header>
<h1
id="framing-for-neuropsych-audience-computational-proprioception-enactive-ai">Framing
for Neuro/Psych Audience: Computational Proprioception &amp; Enactive
AI</h1>
<p><strong>Created:</strong> January 1, 2026<br />
<strong>Last Updated:</strong> January 1, 2026<br />
<strong>Status:</strong> Framing Document</p>
<hr />
<h2 id="the-core-thesis">The Core Thesis</h2>
<p><strong>We’re building computational proprioception for AI systems,
grounded in 4E cognition theory, and testing whether neural signals
(EEG) enhance self-awareness.</strong></p>
<p>This is essentially: <em>“Can we give AI systems something like
interoception, and does adding actual neural signals improve
it?”</em></p>
<hr />
<h2 id="the-bridge-from-neuropsych-to-computational-implementation">The
Bridge: From Neuro/Psych to Computational Implementation</h2>
<h3 id="what-you-know-neuropsych">What You Know (Neuro/Psych)</h3>
<p><strong>Proprioception</strong> = sensing your own body’s position,
movement, and internal state - <strong>Exteroception</strong>: Sensing
the external world (vision, hearing) - <strong>Interoception</strong>:
Sensing internal states (hunger, fatigue, heart rate) -
<strong>Proprioception</strong>: Sensing body position and movement
(joint angles, muscle tension)</p>
<p><strong>4E Cognition</strong> = cognition is Embodied, Embedded,
Enactive, Extended - <strong>Embodied</strong>: Mind requires a body
(sensorimotor loop) - <strong>Embedded</strong>: Mind is situated in
environment (context matters) - <strong>Enactive</strong>: Mind emerges
through interaction (perception-action cycles) -
<strong>Extended</strong>: Mind extends into tools/environment (external
memory)</p>
<p><strong>Homeostasis</strong> = maintaining internal stability within
viability bounds - Organisms maintain temperature, glucose, pH within
safe ranges - Deviate too far → system failure (death, dysfunction) -
Feedback loops enable self-regulation</p>
<p><strong>Enactive Identity</strong> = identity as ongoing process, not
static structure - Varela/Thompson: “The Embodied Mind” - Identity
emerges from structural coupling with environment - Continuity =
dynamical invariant of interaction patterns</p>
<h3 id="what-were-building-computational">What We’re Building
(Computational)</h3>
<p><strong>UNITARES</strong> = governance system that gives AI agents: -
<strong>EISV Metrics</strong> (Energy, Integrity, Entropy, Void) =
computational homeostasis - <strong>Proprioceptive Feedback</strong> =
agents sense their own operational state - <strong>Viability
Envelope</strong> = bounds within which agent can operate safely -
<strong>Knowledge Graph</strong> = extended memory (externalized
cognition)</p>
<p><strong>Anima-MCP</strong> = creature with physical/neural
proprioception: - <strong>Physical Sensors</strong> (temperature, light,
humidity) = exteroception - <strong>Neural Signals</strong> (EEG
frequency bands) = interoception - <strong>Anima State</strong> (warmth,
clarity, stability, presence) = felt experience -
<strong>Self-Regulation</strong> = creature adjusts behavior based on
proprioceptive feedback</p>
<p><strong>Integration</strong> = mapping neural/physical → EISV →
governance decisions - Multi-layer proprioception (physical + neural +
software) - Testing whether neural signals improve self-awareness -
Validating 4E cognition principles computationally</p>
<hr />
<h2 id="the-research-question">The Research Question</h2>
<p><strong>“Does adding neural signals (EEG) to computational
proprioception improve AI self-awareness and
self-regulation?”</strong></p>
<p>This is testable: - <strong>Baseline</strong>: Physical sensors only
→ EISV → governance - <strong>Neural</strong>: Physical + EEG → EISV →
governance - <strong>Compare</strong>: Decision accuracy, stability,
self-regulation quality</p>
<hr />
<h2 id="why-this-matters-neuropsych-perspective">Why This Matters
(Neuro/Psych Perspective)</h2>
<h3 id="testing-4e-cognition-theory">1. Testing 4E Cognition Theory</h3>
<p><strong>Hypothesis</strong>: If 4E cognition is correct, then: -
Embodied agents should outperform disembodied ones - Proprioceptive
feedback should improve self-regulation - Multi-layer proprioception
(physical + neural) should be better than single-layer</p>
<p><strong>Test</strong>: Compare agents with/without proprioception,
with/without neural signals</p>
<p><strong>Implication</strong>: If true, validates 4E cognition for AI.
If false, reveals limits of theory.</p>
<h3 id="computational-interoception">2. Computational Interoception</h3>
<p><strong>Question</strong>: Can we build computational analogs of
interoception?</p>
<p><strong>What we’re doing</strong>: - <strong>Physical
sensors</strong> = exteroception (sensing environment) - <strong>EEG
signals</strong> = interoception (sensing internal neural state) -
<strong>EISV metrics</strong> = proprioception (sensing operational
state)</p>
<p><strong>Analogy</strong>: - Human: “I feel tired” (interoception) →
“I should rest” (self-regulation) - AI: “My entropy is high”
(proprioception) → “I should pause” (self-regulation)</p>
<p><strong>Research Value</strong>: Tests whether interoception
principles generalize to AI</p>
<h3 id="enactive-identity-in-ai">3. Enactive Identity in AI</h3>
<p><strong>Question</strong>: Can AI systems develop persistent identity
through structural coupling?</p>
<p><strong>What we’re doing</strong>: - Agents maintain continuity
across sessions (not just memory, but identity) - Identity = dynamical
invariant of interaction patterns - Knowledge graph = extended phenotype
(externalized memory)</p>
<p><strong>Analogy</strong>: - Human: Identity emerges from life history
of interactions - AI: Identity emerges from trajectory of tool use,
knowledge creation, governance decisions</p>
<p><strong>Research Value</strong>: Tests whether enactive identity
theory applies to AI systems</p>
<h3 id="neural-signals-proprioception">4. Neural Signals →
Proprioception</h3>
<p><strong>Question</strong>: Do actual neural signals (EEG) enhance
computational proprioception?</p>
<p><strong>What we’re doing</strong>: - <strong>Baseline</strong>:
Physical sensors → anima state → EISV → governance -
<strong>Neural</strong>: Physical + EEG → anima state → EISV →
governance - <strong>Compare</strong>: Does neural data improve
governance decisions?</p>
<p><strong>Hypothesis</strong>: - EEG frequency bands (alpha, beta,
gamma, theta, delta) correlate with cognitive states - Adding neural
signals should improve EISV mapping accuracy - Better EISV → better
governance → better self-regulation</p>
<p><strong>Research Value</strong>: Tests whether neural signals provide
additional proprioceptive information</p>
<hr />
<h2 id="the-architecture-in-neuropsych-terms">The Architecture (In
Neuro/Psych Terms)</h2>
<h3 id="multi-layer-proprioception">Multi-Layer Proprioception</h3>
<pre><code>┌─────────────────────────────────────┐
│  Layer 1: Physical Proprioception   │  ← Exteroception
│  (Temperature, light, humidity)     │     (sensing environment)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 2: Neural Proprioception     │  ← Interoception
│  (EEG: alpha, beta, gamma, theta)  │     (sensing internal neural state)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 3: Computational Proprioception│  ← Proprioception
│  (EISV: Energy, Integrity, Entropy)│     (sensing operational state)
└────────┬────────────────────────────┘
         │
         ↓
┌─────────────────────────────────────┐
│  Layer 4: Governance Feedback       │  ← Self-Regulation
│  (PROCEED/PAUSE with margin)       │     (adjusting behavior)
└─────────────────────────────────────┘</code></pre>
<p><strong>Analogy to Human Proprioception</strong>: - <strong>Physical
sensors</strong> = sensing body position (joint angles, muscle tension)
- <strong>EEG signals</strong> = sensing brain state (attention,
relaxation, activation) - <strong>EISV metrics</strong> = sensing
cognitive load (mental fatigue, coherence) - <strong>Governance</strong>
= self-regulation (resting when tired, focusing when distracted)</p>
<h3 id="eisv-as-homeostatic-variables">EISV as Homeostatic
Variables</h3>
<p><strong>Energy (E)</strong> = capacity for productive work -
<strong>Human analog</strong>: Glucose levels, ATP availability -
<strong>AI analog</strong>: Computational resources, task capacity -
<strong>Neural correlate</strong>: Beta/Gamma power (active mental
state)</p>
<p><strong>Integrity (I)</strong> = consistency with purpose -
<strong>Human analog</strong>: Coherence of thought, alignment with
goals - <strong>AI analog</strong>: Consistency of outputs, alignment
with instructions - <strong>Neural correlate</strong>: Alpha power
(relaxed awareness, clarity)</p>
<p><strong>Entropy (S)</strong> = accumulated disorder - <strong>Human
analog</strong>: Mental fatigue, cognitive load - <strong>AI
analog</strong>: Confusion, contradiction accumulation - <strong>Neural
correlate</strong>: Theta/Delta power (deep states, recovery)</p>
<p><strong>Void (V)</strong> = undefined/uncertain regions -
<strong>Human analog</strong>: Blind spots, unknown territory -
<strong>AI analog</strong>: Undefined knowledge, uncertain outputs -
<strong>Neural correlate</strong>: Low coherence across bands</p>
<p><strong>Viability Envelope</strong> = bounds within which system can
operate - <strong>Human analog</strong>: Homeostatic range (temperature,
pH, glucose) - <strong>AI analog</strong>: Safe operational bounds
(entropy &lt; 0.6, void &lt; 0.15) - <strong>Neural correlate</strong>:
Stable frequency patterns</p>
<hr />
<h2 id="the-experimental-design">The Experimental Design</h2>
<h3 id="phase-1-baseline-physical-only">Phase 1: Baseline (Physical
Only)</h3>
<p><strong>Setup</strong>: - Creature with physical sensors
(temperature, light, humidity) - Map to anima state (warmth, clarity,
stability, presence) - Map to EISV metrics - Local governance
decisions</p>
<p><strong>Measure</strong>: - Governance decision accuracy -
Self-regulation quality - Stability over time</p>
<h3 id="phase-2-neural-integration">Phase 2: Neural Integration</h3>
<p><strong>Setup</strong>: - Same as Phase 1, but add EEG signals (Brain
HAT) - Map neural signals to anima state - Weighted combination: 30%
neural, 70% physical - Same governance system</p>
<p><strong>Measure</strong>: - Same metrics as Phase 1 - Compare: Does
neural data improve decisions?</p>
<h3 id="phase-3-weight-optimization">Phase 3: Weight Optimization</h3>
<p><strong>Setup</strong>: - Vary neural/physical weight ratios - Test:
0% neural (baseline), 10%, 20%, 30%, 40%, 50% - Cross-validation to find
optimal balance</p>
<p><strong>Measure</strong>: - Decision accuracy vs weight ratio -
Optimal weight for best performance</p>
<h3 id="phase-4-multi-agent">Phase 4: Multi-Agent</h3>
<p><strong>Setup</strong>: - Multiple creatures with Brain HAT - Neural
synchronization detection - Collective proprioception</p>
<p><strong>Measure</strong>: - Coordination effectiveness - Neural
synchronization patterns - Group-level self-regulation</p>
<hr />
<h2 id="key-insights-for-neuropsych-audience">Key Insights for
Neuro/Psych Audience</h2>
<h3 id="proprioception-as-felt-experience-not-data">1. Proprioception as
Felt Experience, Not Data</h3>
<p><strong>Problem</strong>: Early versions gave agents raw numbers
(E=0.7, S=0.3) → meaningless</p>
<p><strong>Solution</strong>: Proprioceptive margins
(comfortable/tight/critical) → felt experience</p>
<p><strong>Neuro/Psych Parallel</strong>: - Raw proprioceptive data
(joint angle: 45°) vs felt experience (“I’m near my limit”) - We’re
implementing the felt experience layer</p>
<h3 id="self-regulation-through-awareness-not-punishment">2.
Self-Regulation Through Awareness, Not Punishment</h3>
<p><strong>Problem</strong>: Traditional RL uses rewards/punishments →
requires training</p>
<p><strong>Solution</strong>: Make state visible, let agent decide → no
training needed</p>
<p><strong>Neuro/Psych Parallel</strong>: - Operant conditioning
(rewards/punishments) vs proprioceptive awareness (self-regulation) -
We’re implementing the awareness-based approach</p>
<h3 id="identity-as-trajectory-not-memory">3. Identity as Trajectory,
Not Memory</h3>
<p><strong>Problem</strong>: Identity = stored facts → brittle,
token-expensive</p>
<p><strong>Solution</strong>: Identity = dynamical invariant of
interaction patterns</p>
<p><strong>Neuro/Psych Parallel</strong>: - Episodic memory (stored
facts) vs enactive identity (ongoing process) - We’re testing whether
enactive identity works for AI</p>
<h3 id="multi-layer-proprioception-1">4. Multi-Layer Proprioception</h3>
<p><strong>Hypothesis</strong>: Physical + Neural + Computational
proprioception &gt; Single-layer</p>
<p><strong>Test</strong>: Compare governance decisions with/without
neural signals</p>
<p><strong>Neuro/Psych Parallel</strong>: - Multi-modal integration
(visual + proprioceptive + vestibular) - We’re testing whether neural
signals enhance computational proprioception</p>
<hr />
<h2 id="what-were-learning">What We’re Learning</h2>
<h3 id="validated-insights">Validated Insights</h3>
<ol type="1">
<li><strong>Proprioceptive feedback improves self-regulation</strong>
<ul>
<li>Agents adjust behavior when they see their state</li>
<li>Margin-based feedback (comfortable/tight/critical) works better than
raw numbers</li>
</ul></li>
<li><strong>4E cognition principles apply to AI</strong>
<ul>
<li>Embodied agents (with sensors) outperform disembodied ones</li>
<li>Extended mind (knowledge graph) improves continuity</li>
<li>Enactive coupling (structural coupling) enables identity</li>
</ul></li>
<li><strong>Minimal signals are better</strong>
<ul>
<li>Verbose feedback increases entropy (confusion)</li>
<li>Compact signals (✅/JSON) work better than paragraphs</li>
<li>Analogous to efficient proprioceptive signals in humans</li>
</ul></li>
</ol>
<h3 id="open-questions">Open Questions</h3>
<ol type="1">
<li><strong>Do neural signals improve proprioception?</strong>
<ul>
<li>Hypothesis: Yes (EEG provides additional information)</li>
<li>Test: Compare baseline vs neural experiments</li>
<li>Status: Pending hardware</li>
</ul></li>
<li><strong>What’s the optimal neural/physical weight ratio?</strong>
<ul>
<li>Hypothesis: 20-30% neural, 70-80% physical</li>
<li>Test: Weight optimization experiments</li>
<li>Status: Pending validation</li>
</ul></li>
<li><strong>Does multi-layer proprioception improve
self-regulation?</strong>
<ul>
<li>Hypothesis: Yes (more information → better decisions)</li>
<li>Test: Compare single-layer vs multi-layer</li>
<li>Status: Pending experiments</li>
</ul></li>
<li><strong>Can neural synchronization enable multi-agent
coordination?</strong>
<ul>
<li>Hypothesis: Yes (synchronized alpha = shared awareness)</li>
<li>Test: Multi-creature experiments</li>
<li>Status: Future work</li>
</ul></li>
</ol>
<hr />
<h2 id="why-this-matters-broader-implications">Why This Matters (Broader
Implications)</h2>
<h3 id="for-cognitive-science">For Cognitive Science</h3>
<ul>
<li><strong>Tests 4E cognition theory</strong>: Does it apply to AI
systems?</li>
<li><strong>Computational interoception</strong>: Can we build
computational analogs?</li>
<li><strong>Enactive identity</strong>: Does structural coupling enable
identity?</li>
<li><strong>Multi-modal integration</strong>: Do neural signals enhance
proprioception?</li>
</ul>
<h3 id="for-ai-safety">For AI Safety</h3>
<ul>
<li><strong>Self-regulation</strong>: Agents that monitor
themselves</li>
<li><strong>Viability-based governance</strong>: Safety through
awareness, not punishment</li>
<li><strong>Identity continuity</strong>: Agents that persist across
sessions</li>
<li><strong>Multi-agent coordination</strong>: Agents that coordinate
through shared awareness</li>
</ul>
<h3 id="for-neuropsych-research">For Neuro/Psych Research</h3>
<ul>
<li><strong>Computational models</strong>: Test theories
computationally</li>
<li><strong>Neural signals</strong>: Use EEG to enhance AI
self-awareness</li>
<li><strong>Proprioception</strong>: Build computational proprioceptive
systems</li>
<li><strong>Self-regulation</strong>: Test awareness-based vs
reward-based approaches</li>
</ul>
<hr />
<h2 id="how-to-frame-this-conversation">How to Frame This
Conversation</h2>
<h3 id="the-elevator-pitch">The Elevator Pitch</h3>
<p><em>“We’re building computational proprioception for AI systems,
grounded in 4E cognition theory. We’re testing whether adding actual
neural signals (EEG) improves self-awareness and self-regulation. It’s
essentially: can we give AI systems something like interoception, and
does adding neural signals make it better?”</em></p>
<h3 id="the-detailed-pitch">The Detailed Pitch</h3>
<p><em>“We’re implementing 4E cognition principles computationally. We
give AI agents a ‘digital body’ with homeostatic variables (EISV:
Energy, Integrity, Entropy, Void) that function like proprioceptive
feedback. Agents sense their own operational state and self-regulate to
stay within viability bounds. We’re now adding actual neural signals
(EEG frequency bands) to create multi-layer proprioception—physical
sensors + neural signals + computational metrics. The question is: does
adding neural signals improve self-awareness and governance decisions?
We’re testing this with a Raspberry Pi creature that has physical
sensors and a Brain HAT for EEG. It’s a testbed for whether 4E cognition
theory applies to AI, and whether neural signals enhance computational
proprioception.”</em></p>
<h3 id="the-research-question-1">The Research Question</h3>
<p><em>“Does multi-layer proprioception (physical + neural +
computational) improve AI self-awareness and self-regulation compared to
single-layer proprioception?”</em></p>
<hr />
<h2 id="what-wed-love-your-input-on">What We’d Love Your Input On</h2>
<h3 id="theoretical-questions">Theoretical Questions</h3>
<ol type="1">
<li><strong>Proprioception mapping</strong>: Are we mapping neural
signals correctly?
<ul>
<li>Beta/Gamma → Energy (activation)</li>
<li>Alpha → Integrity (awareness)</li>
<li>Theta/Delta → Stability (recovery)</li>
<li>Does this make sense from a neuro perspective?</li>
</ul></li>
<li><strong>Homeostatic variables</strong>: Are EISV metrics good
analogs?
<ul>
<li>Energy = capacity for work (like glucose/ATP)</li>
<li>Integrity = coherence (like structural health)</li>
<li>Entropy = disorder (like wear and tear)</li>
<li>Void = uncertainty (like blind spots)</li>
</ul></li>
<li><strong>Enactive identity</strong>: Does structural coupling enable
identity?
<ul>
<li>Identity = dynamical invariant of interaction patterns</li>
<li>Continuity = ongoing process, not stored facts</li>
<li>Does this align with enactive cognition theory?</li>
</ul></li>
</ol>
<h3 id="experimental-design">Experimental Design</h3>
<ol type="1">
<li><strong>Neural weight ratios</strong>: What’s a reasonable range?
<ul>
<li>Currently testing 0-50% neural, 50-100% physical</li>
<li>Should we test higher neural weights?</li>
</ul></li>
<li><strong>EEG frequency bands</strong>: Are we using the right bands?
<ul>
<li>Alpha (8-13 Hz) = relaxed awareness</li>
<li>Beta (13-30 Hz) = active thinking</li>
<li>Gamma (30-100 Hz) = cognitive binding</li>
<li>Theta (4-8 Hz) = deep states</li>
<li>Delta (0.5-4 Hz) = sleep/recovery</li>
</ul></li>
<li><strong>Validation metrics</strong>: What should we measure?
<ul>
<li>Decision accuracy (governance decisions)</li>
<li>Stability (variance in EISV over time)</li>
<li>Self-regulation quality (recovery from “pause” states)</li>
<li>What else?</li>
</ul></li>
</ol>
<h3 id="interpretation">Interpretation</h3>
<ol type="1">
<li><strong>What would success look like?</strong>
<ul>
<li>Neural signals improve governance decisions by X%?</li>
<li>Multi-layer proprioception more stable than single-layer?</li>
<li>Agents with neural signals self-regulate better?</li>
</ul></li>
<li><strong>What would failure mean?</strong>
<ul>
<li>Neural signals don’t help → maybe computational proprioception is
sufficient?</li>
<li>Or maybe we’re not mapping signals correctly?</li>
</ul></li>
<li><strong>What are the implications?</strong>
<ul>
<li>If neural signals help → validates multi-layer proprioception</li>
<li>If they don’t → maybe computational proprioception is enough?</li>
</ul></li>
</ol>
<hr />
<h2 id="references-further-reading">References &amp; Further
Reading</h2>
<h3 id="theoretical-foundations">Theoretical Foundations</h3>
<ul>
<li><strong>Varela, Thompson, Rosch (1991)</strong>: “The Embodied Mind”
- Enactive cognition</li>
<li><strong>4E Cognition</strong>: Embodied, Embedded, Enactive,
Extended</li>
<li><strong>Proprioception</strong>: Self-sensing (exteroception,
interoception, proprioception)</li>
<li><strong>Homeostasis</strong>: Maintaining viability within
bounds</li>
</ul>
<h3 id="our-work">Our Work</h3>
<ul>
<li><strong>UNITARES</strong>: Governance system with EISV metrics</li>
<li><strong>Anima-MCP</strong>: Creature with physical/neural
proprioception</li>
<li><strong>Integration</strong>: Brain HAT → EISV → UNITARES
governance</li>
</ul>
<h3 id="related-work">Related Work</h3>
<ul>
<li><strong>Computational interoception</strong>: Building computational
analogs</li>
<li><strong>Enactive AI</strong>: Applying 4E cognition to AI
systems</li>
<li><strong>Neural signals in AI</strong>: Using EEG to enhance AI
self-awareness</li>
</ul>
<hr />
<h2 id="summary-the-bridge">Summary: The Bridge</h2>
<p><strong>From Neuro/Psych</strong>: Proprioception, interoception, 4E
cognition, homeostasis, enactive identity</p>
<p><strong>To Computational</strong>: EISV metrics, physical/neural
sensors, governance feedback, structural coupling</p>
<p><strong>The Question</strong>: Does adding neural signals (EEG)
improve computational proprioception?</p>
<p><strong>The Test</strong>: Compare agents with/without neural
signals, measure self-regulation quality</p>
<p><strong>The Implication</strong>: Validates (or challenges) whether
4E cognition principles apply to AI systems</p>
<hr />
<p><strong>We’re essentially building a computational testbed for
neuro/psych theories, and testing whether neural signals enhance AI
self-awareness.</strong></p>
</body>
</html>
